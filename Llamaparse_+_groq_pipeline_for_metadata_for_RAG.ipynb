{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokyo8182/LLM-RAG/blob/main/Llamaparse_%2B_groq_pipeline_for_metadata_for_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Read in all files i.e. data ingestion\n",
        "\n",
        "Basic csv reading code added. Next: use llamaparse's own data loaders"
      ],
      "metadata": {
        "id": "Rdgxge09cp9l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZNRp0Cs7DQH",
        "outputId": "f2bf7dda-989d-421c-a162-b86f13a09620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connecting to google drive drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "id": "fJRuUhDn8zGj",
        "outputId": "8795e843-765f-4c5e-ce42-58817c4c78d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Omdena_Challenge/new_LK_tea_dataset/new_LK_tea_dataset_updated.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7d1ca4da73d9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Read the Google Sheet into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Display the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Omdena_Challenge/new_LK_tea_dataset/new_LK_tea_dataset_updated.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# The CSV download link\n",
        "path = '/content/drive/MyDrive/Omdena_Challenge/new_LK_tea_dataset/new_LK_tea_dataset_updated.csv'\n",
        "\n",
        "# Read the Google Sheet into a pandas DataFrame\n",
        "all_data = pd.read_csv(path , index_col=0)\n",
        "all_data.fillna('', inplace=True)\n",
        "# Display the DataFrame\n",
        "all_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to count words and characters in text\n",
        "def count_words_and_characters(text):\n",
        "    words = text.split()  # Split by whitespace to get words\n",
        "    num_words = len(words)\n",
        "    num_characters = len(text)\n",
        "    return num_words, num_characters\n",
        "\n",
        "# Function to count words and characters for the 'text_content' column in the DataFrame\n",
        "def count_stats_in_dataframe(df):\n",
        "    total_words = 0\n",
        "    total_characters = 0\n",
        "\n",
        "    # Iterate over the 'text_content' column in the DataFrame\n",
        "    for text in df['text_content'].dropna():  # Drop any NaN values in text_content\n",
        "        num_words, num_characters = count_words_and_characters(text)\n",
        "        total_words += num_words\n",
        "        total_characters += num_characters\n",
        "\n",
        "    return total_words, total_characters\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Count words and characters\n",
        "    total_words, total_characters = count_stats_in_dataframe(all_data)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nTotal Stats for 'text_content' column in DataFrame:\")\n",
        "    print(f\"Total Words: {total_words}\")\n",
        "    print(f\"Total Characters: {total_characters}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4361cea8-b713-45e4-8c3a-15a404e1f835",
        "id": "MPrzBsUJ-rH2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Stats for 'text_content' column in DataFrame:\n",
            "Total Words: 186342\n",
            "Total Characters: 1120744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # for th future, can use paths to check where path ends in .pdf\n",
        "# # Filter rows where the 'path' column ends with '.pdf'\n",
        "\n",
        "# data_pdf = all_data[all_data['PDF_or_text'] == 'PDF']"
      ],
      "metadata": {
        "id": "mBatXcQn-rH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # modify to add text_only in either path or new column\n",
        "# data_text = all_data[all_data['PDF_or_text'] == 'Text']\n"
      ],
      "metadata": {
        "id": "v-8o4TWp-rH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku6oTYYFXxCv"
      },
      "source": [
        "# 2. Feed all the PDF files as well as ACTs to llamaparse and get parsed chunks in markdown format\n",
        "\n",
        "for 173 files, around 600 chunks are returned"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@TODO convert ACTs via Fast mode (1 cred. / 3 p)\n",
        "@TODO convert PDFs via Accurate mode (1 cred. / 1 p.), and don't use continuous mode (10 credits)...except for PDFs with Tables....becuase if they span multiple pages, it would be an issue....................."
      ],
      "metadata": {
        "id": "oIjg9KhiouY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install llama-index-core llama-parse llama-index-readers-file python-dotenv"
      ],
      "metadata": {
        "id": "aq7j19w2AyHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72329795-164e-452b-eda2-27c31f9804b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9f9ac6-d56d-4439-be6c-210c232fe6ad",
        "id": "a3D8dwXzXxCy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LLamacloud API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['LLAMA_CLOUD_API_KEY'] = getpass.getpass('Enter your LLamacloud API Key: ')\n",
        "# os.environ['GROQ_API_KEY'] = getpass.getpass('Enter your GROQ API Key: ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "l-qW5aowBotn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my version\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# Load environment variables (assuming LLAMA_CLOUD_API_KEY is needed somewhere)\n",
        "load_dotenv()\n",
        "\n",
        "# Assuming you have your DataFrame `all_data` already loaded\n",
        "# Example: all_data = pd.read_csv(\"your_data.csv\") or whatever source you're using\n",
        "\n",
        "# Prepare LlamaParse instance for PDF file that uses accurate mode which is true by default:\n",
        "# it hallucinates so changing prompt\n",
        "\n",
        "# pdf_parser = LlamaParse(result_type='text', verbose=True, continuous_mode=True, language= 'en',\n",
        "#                     parsing_instruction = \"You are parsing a regulatory document related to the Sri Lankan Tea Industry. Please give me a json with the title and date of issuance of the document\"\n",
        "#                     )  # \"markdown\" and \"text\" are available\n",
        "\n",
        "\n",
        "# also doesn't work, don't use parsing prompt\n",
        "pdf_parser = LlamaParse(result_type='markdown', num_workers=8)  # \"markdown\" and \"text\" are available\n",
        "\n",
        "# Prepare LlamaParse instance for text file that uses fast mode:\n",
        "text_parser = LlamaParse(result_type='markdown', Set_fast_mode = True, num_workers=8)  # \"markdown\" and \"text\" are available\n",
        "\n",
        "# set workers =8 makes it 'fake' parallel\n",
        "\n",
        "# default is text, modified to markdown for next time. Also, changed number of workers to 8 for next time\n"
      ],
      "metadata": {
        "id": "jYA4a_GLDFdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [del]\n",
        "# # apparently this is synchronous batch parsing, extremely inefficient\n",
        "# import pandas as pd\n",
        "# from llama_parse import LlamaParse\n",
        "# from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "\n",
        "# # This function will handle processing each document based on its type\n",
        "# def process_documents(row):\n",
        "#     documents = []  # Initialize documents list locally for each row\n",
        "#     file_type = row['PDF_or_text']  # Column that indicates if it's 'PDF' or 'text'\n",
        "\n",
        "#     # If the file type is 'PDF', process with the parser\n",
        "#     if file_type == 'PDF':\n",
        "#         file_path = row['path']  # Assuming each row has a 'path' column\n",
        "#         file_extractor = {\".pdf\": pdf_parser}\n",
        "#         pdf_documents = SimpleDirectoryReader(input_files=[file_path], file_extractor=file_extractor).load_data()\n",
        "\n",
        "#         # Append the processed PDF data to the documents list\n",
        "#         # documents.extend(pdf_documents)\n",
        "#         return pdf_documents\n",
        "#     # If the file type is 'Text', process the text file\n",
        "#     elif file_type == 'Text':\n",
        "#         file_path = row['text_path']  # Assuming each row has a 'text_path' column\n",
        "#         file_extractor = {\".txt\": text_parser}\n",
        "#         text_documents = SimpleDirectoryReader(input_files=[file_path], file_extractor=file_extractor).load_data()\n",
        "#         # @TODO document = Document(\n",
        "#         # text=\"text\",\n",
        "#         # metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
        "#         # )\n",
        "#         # Append the processed text data to the documents list\n",
        "#         # documents.extend(text_documents)\n",
        "#         return text_documents\n",
        "#     # If the file type is neither 'PDF' nor 'Text', return an empty list\n",
        "#     else:\n",
        "#       return None  # Return the processed documents for the row\n",
        "\n",
        "# # Apply the process_documents function to each row of the DataFrame\n",
        "# all_data['processed_data'] = all_data.apply(process_documents, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sxo9jiinGdpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # works perfectly but no metadata, opened an issue here: https://github.com/run-llama/llama_parse/issues/493\n",
        "# import pandas as pd\n",
        "# from llama_parse import LlamaParse\n",
        "# import asyncio\n",
        "\n",
        "# # Initialize parsers\n",
        "# pdf_parser = LlamaParse(result_type='markdown', num_workers=8)\n",
        "# text_parser = LlamaParse(result_type='markdown', Set_fast_mode=True, num_workers=8)\n",
        "\n",
        "\n",
        "# # Async function to process PDF documents\n",
        "# async def process_pdf_documents(pdf_paths):\n",
        "#     if pdf_paths:  # Only process if there are PDF paths\n",
        "#         documents = await pdf_parser.aload_data(pdf_paths)\n",
        "#         return documents\n",
        "#     return []\n",
        "\n",
        "# # Async function to process Text documents\n",
        "# async def process_text_documents(text_paths):\n",
        "#     if text_paths:  # Only process if there are text paths\n",
        "#         documents = await text_parser.aload_data(text_paths)\n",
        "#         return documents\n",
        "#     return []\n",
        "\n",
        "# # Async function to process the DataFrame and save to CSV\n",
        "# async def process_and_save_df(all_data):\n",
        "#     # Collect file paths for PDFs and Text documents\n",
        "#     pdf_paths = all_data[all_data['PDF_or_text'] == 'PDF']['path'].tolist()\n",
        "#     text_paths = all_data[all_data['PDF_or_text'] == 'Text']['text_path'].tolist()\n",
        "\n",
        "#     # Process PDFs and Texts concurrently\n",
        "#     pdf_documents = await process_pdf_documents(pdf_paths)\n",
        "#     text_documents = await process_text_documents(text_paths)\n",
        "\n",
        "#     # Combine the results\n",
        "#     all_documents = pdf_documents + text_documents\n",
        "\n",
        "#     if all_documents:\n",
        "#         return all_documents\n",
        "#     else:\n",
        "#         print(\"No documents found for processing.\")\n",
        "#         return []\n",
        "\n",
        "# # Example usage\n",
        "# async def main():\n",
        "#     # Assuming `all_data` is a DataFrame that includes columns like 'PDF_or_text', 'path', 'text_path'\n",
        "#     # all_data = pd.DataFrame({\n",
        "#     #     'PDF_or_text': ['PDF', 'Text', 'PDF', 'Text'],\n",
        "#     #     'path': ['./data/file1.pdf', '', './data/file3.pdf', ''],\n",
        "#     #     'text_path': ['', './data/file2.txt', '', './data/file4.txt']\n",
        "#     # })\n",
        "\n",
        "#     # Process documents and get the list of processed documents\n",
        "#     processed_documents = await process_and_save_df(all_data)\n",
        "\n",
        "#     if processed_documents:\n",
        "#         return processed_documents\n",
        "#     else:\n",
        "#         print(\"No documents to process.\")\n",
        "\n",
        "# # Run the asynchronous main function\n",
        "# processed_documents = asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sysd7G5JSmCW",
        "outputId": "cb38e3c7-75c2-41d3-e84c-9393d8e3a3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing files: 100%|██████████| 143/143 [01:43<00:00,  1.38it/s]\n",
            "Parsing files: 100%|██████████| 29/29 [00:42<00:00,  1.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_parse import LlamaParse\n",
        "import asyncio\n",
        "\n",
        "# Initialize parsers\n",
        "pdf_parser = LlamaParse(result_type='markdown', num_workers=8)\n",
        "text_parser = LlamaParse(result_type='markdown', Set_fast_mode=True, num_workers=8)\n",
        "\n",
        "\n",
        "# Async function to process PDF documents\n",
        "async def process_pdf_documents(pdf_paths):\n",
        "    if pdf_paths:  # Only process if there are PDF paths\n",
        "        file_extractor = {\".pdf\": pdf_parser}\n",
        "        pdf_documents = SimpleDirectoryReader(input_files=pdf_paths, file_extractor=file_extractor).load_data()\n",
        "\n",
        "        return pdf_documents\n",
        "    return []\n",
        "\n",
        "# Async function to process Text documents\n",
        "async def process_text_documents(text_paths):\n",
        "    if text_paths:  # Only process if there are text paths\n",
        "        file_extractor = {\".txt\": text_parser}\n",
        "        text_documents = SimpleDirectoryReader(input_files=text_paths, file_extractor=file_extractor).load_data()\n",
        "        return text_documents\n",
        "    return []\n",
        "\n",
        "# Async function to process the DataFrame and save to CSV\n",
        "async def process_and_save_df(all_data):\n",
        "    # Collect file paths for PDFs and Text documents\n",
        "    pdf_paths = all_data[all_data['PDF_or_text'] == 'PDF']['path'].tolist()\n",
        "    text_paths = all_data[all_data['PDF_or_text'] == 'Text']['text_path'].tolist()\n",
        "\n",
        "    # Process PDFs and Texts concurrently\n",
        "    pdf_documents = await process_pdf_documents(pdf_paths)\n",
        "    text_documents = await process_text_documents(text_paths)\n",
        "\n",
        "    # Combine the results\n",
        "    all_documents = pdf_documents + text_documents\n",
        "\n",
        "    if all_documents:\n",
        "        return all_documents\n",
        "    else:\n",
        "        print(\"No documents found for processing.\")\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "async def main():\n",
        "    # Assuming `all_data` is a DataFrame that includes columns like 'PDF_or_text', 'path', 'text_path'\n",
        "    # all_data = pd.DataFrame({\n",
        "    #     'PDF_or_text': ['PDF', 'Text', 'PDF', 'Text'],\n",
        "    #     'path': ['./data/file1.pdf', '', './data/file3.pdf', ''],\n",
        "    #     'text_path': ['', './data/file2.txt', '', './data/file4.txt']\n",
        "    # })\n",
        "\n",
        "    # Process documents and get the list of processed documents\n",
        "    processed_documents = await process_and_save_df(all_data)\n",
        "\n",
        "    if processed_documents:\n",
        "        return processed_documents\n",
        "    else:\n",
        "        print(\"No documents to process.\")\n",
        "\n",
        "# Run the asynchronous main function\n",
        "processed_documents = asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zslNUm9xbx5E",
        "outputId": "277c3463-9d5c-4b7a-e4fc-75e275c7de25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 8d0003a2-989f-48fe-a9bb-5bf519673547\n",
            "Started parsing the file under job_id 58a2e3a9-ed28-448f-89b1-e3273675c842\n",
            "Started parsing the file under job_id e578ce21-8806-42cf-a9a1-dde7a951a9d3\n",
            "Started parsing the file under job_id a76296cd-ee5a-463c-97aa-c3cd8f867667\n",
            "Started parsing the file under job_id c861ae1d-db83-4275-abf1-eb5693fb7636\n",
            "Started parsing the file under job_id 78e8be11-80ee-46ca-9145-fa4621953e28\n",
            "Started parsing the file under job_id ea63608c-1289-410b-a799-3a12ae56becf\n",
            "Started parsing the file under job_id 4b1e4b89-f6de-4627-b207-3395832822a7\n",
            "Started parsing the file under job_id a7e0315a-c073-4f3d-b063-58491f5068db\n",
            "Started parsing the file under job_id 1ceb5eeb-c50d-442d-b6dd-fd5e952d4ef0\n",
            "Started parsing the file under job_id 5fac58c1-3d88-4c34-b8cd-cb05ac2def85\n",
            "Started parsing the file under job_id fc58feda-9532-4b08-b5e5-694219a405f8\n",
            "Started parsing the file under job_id dee46ec5-bc89-4db6-8b06-d6aebc2685d2\n",
            "Started parsing the file under job_id c4ac950e-c46c-48f7-b2e3-ee751ac2668a\n",
            "Started parsing the file under job_id cff100df-4ba7-4bf7-9b71-cb75e1f897c7\n",
            "Started parsing the file under job_id 136d5f9c-1433-44f9-98a8-5f740f6d59e2\n",
            "Started parsing the file under job_id c5e51c49-0b13-4514-aee4-887661e2aafd\n",
            "Started parsing the file under job_id 495682da-ad42-4a06-9f6f-bd831ea89fd0\n",
            "Started parsing the file under job_id 0852e7db-770c-4525-934d-44077b18e028\n",
            "Started parsing the file under job_id 124741f3-e476-42e0-9b1a-8352c6bad1b2\n",
            "Started parsing the file under job_id 0119b429-f3eb-41ad-b99f-2b121404f4e2\n",
            "Started parsing the file under job_id 068a9dcb-93ca-4f31-8da4-37ce910815cb\n",
            "Started parsing the file under job_id 196eb25c-45db-44ef-bb82-f1e09ecc73ac\n",
            "Started parsing the file under job_id c54362db-36bb-4995-8dbc-10b28e764cf9\n",
            "Started parsing the file under job_id 76d05d35-4bc4-404a-96be-77a11b79214b\n",
            "Started parsing the file under job_id eb044d16-3543-48ac-9164-3b06b168bc89\n",
            "Started parsing the file under job_id 1403ea36-4676-4f12-9e68-e584b638cb5e\n",
            "Started parsing the file under job_id f745d66d-7656-4501-a349-0fb9de199914\n",
            "Started parsing the file under job_id 5b81bcec-3872-422c-b356-6f83efbdcc6b\n",
            "Started parsing the file under job_id 203f8b93-af67-4be8-be5d-804bcd204576\n",
            "Started parsing the file under job_id bf77f0a5-1c2f-42bd-a175-db14b21273f2\n",
            "Started parsing the file under job_id 0e1b4f4f-07be-494b-b327-316337a443bb\n",
            "Started parsing the file under job_id 436a671e-a423-4b9c-bcf9-b21ed984cf34\n",
            "Started parsing the file under job_id 4a996937-6c40-4e0c-b7c3-69445fd25ea6\n",
            "Started parsing the file under job_id 8f839293-7fc1-483b-9f62-6721c43a831d\n",
            "Started parsing the file under job_id 29d6278b-01f7-445e-b18c-9af8632264ad\n",
            "Started parsing the file under job_id c3bd3abe-31e8-4099-ad33-9f74557a3317\n",
            "Started parsing the file under job_id 2c5f0d26-c0cd-4295-acfc-6ea53763a683\n",
            "Started parsing the file under job_id 40ad0f71-d553-4e96-8765-28e5728d3c45\n",
            "Started parsing the file under job_id 2ac393f8-d2b3-47ef-81cb-f4c8290e8b1b\n",
            "Started parsing the file under job_id 9e24dbd6-ccf5-4fa2-855d-d804b888c17c\n",
            "Started parsing the file under job_id 7292f6f9-37b2-486d-9bb3-8227c08aa678\n",
            "Started parsing the file under job_id 1eb771ef-c98a-42d7-9d72-0290e4cdaf4f\n",
            "Started parsing the file under job_id df171874-7092-4556-a908-9d977fbbfccc\n",
            "Started parsing the file under job_id 11066322-ba78-48a5-b5a1-40198c6a974f\n",
            "Started parsing the file under job_id 1ac5dc67-e02c-4445-8a41-9d9ffc246931\n",
            "Started parsing the file under job_id fa6b91b0-96e1-480c-8d82-f00877df73d2\n",
            "Started parsing the file under job_id b8c5cc45-57bb-458f-a8b2-3853d02e67af\n",
            "Started parsing the file under job_id ed0b7cbb-fc90-4fd7-b6d5-eb1c19744c3c\n",
            "Started parsing the file under job_id 4717025b-bd65-4faf-a57f-5da973fa3096\n",
            "Started parsing the file under job_id a7331042-fdba-4726-a67d-2e720846e72b\n",
            "Started parsing the file under job_id 9d02347c-80ac-42cc-912c-2789fbd4481e\n",
            "Started parsing the file under job_id 6c873db2-dd59-4e5b-a7dc-84a1a03164f9\n",
            "Started parsing the file under job_id f87ee570-5b75-44b2-870f-7e468fa59eab\n",
            "Started parsing the file under job_id 5254db79-034c-4727-beb5-895b2bd18d60\n",
            "Started parsing the file under job_id c4a79f04-9d7a-46e2-8eb8-6e3bef495ebc\n",
            "Started parsing the file under job_id 42fb29f0-5fa0-4f8c-b685-b15ed719b63e\n",
            "Started parsing the file under job_id 2f4ad93b-b877-41c5-85ed-10358a5de754\n",
            "Started parsing the file under job_id fc7ac789-2672-4d13-8cca-01f6e10729ed\n",
            "Started parsing the file under job_id 4548a360-e1d6-4bfe-8069-4bb5d8b0317b\n",
            "Started parsing the file under job_id fff8453c-9f3f-43a0-a17b-6caad30d7d4d\n",
            "Started parsing the file under job_id ec135d0f-38bb-4ed7-b36a-36888f4ce0f9\n",
            "Started parsing the file under job_id 9c4aba1b-ccad-41b0-bf1c-5732ab440d8d\n",
            "Started parsing the file under job_id d020c23a-375d-4374-8880-205d84a29b3b\n",
            "Started parsing the file under job_id 0f2c7c9f-9e5b-41cb-a20f-91397dc83c89\n",
            "Started parsing the file under job_id 39c1e1d4-5d0f-4787-abab-1aa555162bee\n",
            "Started parsing the file under job_id b7916c39-c998-47e4-88d5-401efe5a245a\n",
            "Started parsing the file under job_id 193a3433-d863-49a2-a2c6-4e2eb345af91\n",
            "Started parsing the file under job_id 78f4d931-f7f6-436f-ae8a-9d6576ee6fa9\n",
            "Started parsing the file under job_id a5a3dd8b-21db-40f6-a047-4dfd76a62ffd\n",
            "Started parsing the file under job_id ce921757-2cfd-4d35-b407-f15734ece46d\n",
            "Started parsing the file under job_id 39ae896e-d113-4344-9cbb-509309c5ec8a\n",
            "Started parsing the file under job_id fcf04002-d1eb-4146-849a-bde9cfc54f0b\n",
            "Started parsing the file under job_id 5b511580-882a-499b-b1eb-301923dd330e\n",
            "Started parsing the file under job_id 976cfac0-e482-41d6-b7a2-c09a09ee61c0\n",
            "Started parsing the file under job_id 6b1fa8b2-08ad-461b-b10b-3c1c28657a7e\n",
            "Started parsing the file under job_id 7a756b12-f6a0-49d1-a6f5-a2a2287bbb50\n",
            "Started parsing the file under job_id 9eeeebad-8927-4663-a3e7-cceef40dd11c\n",
            "Started parsing the file under job_id 42c88bca-253e-4488-b012-f5a257992466\n",
            ".Started parsing the file under job_id 27742e0c-13bb-4642-a423-773e81fd431c\n",
            "Started parsing the file under job_id 013885d2-6477-44c5-ba1e-5752403b1031\n",
            "Started parsing the file under job_id 01663cfb-b705-47d6-9fd8-803e3687c0aa\n",
            "Started parsing the file under job_id f66f216d-bf09-4258-9cdc-9131c27a03d7\n",
            "Started parsing the file under job_id c81f37d4-734e-481f-880a-cd49808da558\n",
            "Started parsing the file under job_id d8438013-012a-40eb-a72c-812c53649ff2\n",
            "Started parsing the file under job_id 367054f7-4982-49e8-8bcf-3685db070e34\n",
            "Started parsing the file under job_id d19cf2a8-adf5-4006-a8c4-6d6c5f8041bd\n",
            "Started parsing the file under job_id 13a2c47f-40c1-4b42-bf8b-aa2a48103370\n",
            "Started parsing the file under job_id 50d7344e-0663-4704-bc08-0a7b39ed1fa1\n",
            "Started parsing the file under job_id 699cb495-5dce-4942-95d5-188b39760ce2\n",
            "Started parsing the file under job_id 1d48d100-2cc8-4b97-9a6e-7866d3ef07d3\n",
            "Started parsing the file under job_id 0ae16b43-74c4-4bed-be8b-489576ded522\n",
            "Started parsing the file under job_id 847e2771-1422-4e3a-b3ff-32cd56602aa3\n",
            "Started parsing the file under job_id 4cbb95e4-3401-4331-a544-cba21837ecb1\n",
            "Started parsing the file under job_id fe22d0eb-47ce-4f22-bb86-3f7512027728\n",
            "Started parsing the file under job_id e57f1db5-f9ac-4bbb-8fd6-394f88d38198\n",
            "Started parsing the file under job_id c818f394-e720-4a2e-9026-c6fbac099b60\n",
            "Started parsing the file under job_id 615186e7-56f4-4dc5-a710-d8fc9407351b\n",
            "Started parsing the file under job_id c3ac75e6-c1e2-4125-9446-600a1ed4ddb5\n",
            "Started parsing the file under job_id 3c8f3ade-90cd-45b0-a8dd-d69c41e4fc6d\n",
            "Started parsing the file under job_id 0defe460-811a-4149-ad83-78611753fb4a\n",
            "Started parsing the file under job_id 2256d7da-b0bd-4dc0-9be0-b54b12c34822\n",
            "Started parsing the file under job_id 058743f0-ad11-47f2-9ef6-7f2f0d92b63d\n",
            "Started parsing the file under job_id bf94f6b6-7ae8-4f24-a6ea-234edb746ce9\n",
            ".Started parsing the file under job_id df2beae3-dc58-405f-8965-add32569cd0d\n",
            "Started parsing the file under job_id bb0fb498-4e1b-4920-9eb0-5538bfc6356e\n",
            "Started parsing the file under job_id 76acf637-aadb-46c3-892d-edca396c4bd0\n",
            "Started parsing the file under job_id 3346d3ac-3006-4795-a6b7-9e2b5b96c9bf\n",
            "Started parsing the file under job_id be41f1ec-5af2-40cb-84e0-2e75d1e05db9\n",
            "Started parsing the file under job_id ce35be97-2ae5-4655-a868-260e8ffe9987\n",
            "Started parsing the file under job_id f1cc9897-34dd-4bd5-b4db-966d022fd910\n",
            "Started parsing the file under job_id c45fb538-137e-487e-a18f-15b53894e475\n",
            "Started parsing the file under job_id bd0b4709-31ee-481a-9a47-5b46b4dcdeb2\n",
            "Started parsing the file under job_id 542f7198-43de-4c3b-b9da-d64a0907578c\n",
            "Started parsing the file under job_id 32fea66d-47ac-4f8a-9ce5-42b79f12b1ad\n",
            "Started parsing the file under job_id e6fb5875-5983-4bd2-8dc7-d0e4d4d936e2\n",
            "Started parsing the file under job_id d5672cf3-82fe-4b23-9b1f-745bca5f7eaa\n",
            "Started parsing the file under job_id 947baede-4074-4897-9b88-1c6f2f4f4a4c\n",
            "Started parsing the file under job_id d879d51b-87cf-40c0-af66-5f11d1008cbf\n",
            "Started parsing the file under job_id c6638a59-12a3-4948-aa54-bd91ec902ca9\n",
            "Started parsing the file under job_id 221f2088-48a1-4db6-8eb8-62d54f22ee2b\n",
            "Started parsing the file under job_id e4471907-4518-4f87-a605-c6b518166cca\n",
            "Started parsing the file under job_id cc3ab796-0818-4b1c-9a45-6ed12613d932\n",
            "Started parsing the file under job_id c7a30e10-eb29-4b44-9252-024eb191cc3a\n",
            "Started parsing the file under job_id 13f10619-aaa3-4c07-b9dc-567abdaf69ac\n",
            "Started parsing the file under job_id 012089c2-71f6-4beb-9ab8-e1ddb110745b\n",
            "Started parsing the file under job_id 9d27eac1-0e66-4ae4-b383-224145d2ab79\n",
            "Started parsing the file under job_id e8f95e4f-957f-448d-b9b8-c2e080c78d8d\n",
            "Started parsing the file under job_id edae1c19-5740-4cfb-befc-771997763afe\n",
            "Started parsing the file under job_id 5f5d2e5e-f445-40f7-9a55-b4e9daa55287\n",
            "Started parsing the file under job_id 104e79ff-bbdf-4782-81aa-e76eaa092b20\n",
            "Started parsing the file under job_id 79c8bec4-4aa2-4b60-a87c-347482b54dd9\n",
            "Started parsing the file under job_id 9c198cbd-d985-417c-9f7e-c4cd016f1c6b\n",
            ".Started parsing the file under job_id 768abe1a-9910-4f36-9c81-8d4f6d904df0\n",
            "Started parsing the file under job_id 6d290e69-8a62-45ee-a1e3-129ed2db8f1c\n",
            "Started parsing the file under job_id 02ad59fe-6dba-483c-b75a-ad44338dba48\n",
            "Started parsing the file under job_id 954be895-dffe-4feb-8d49-79dbd0687344\n",
            "Started parsing the file under job_id 66e0eb5f-14be-40c9-81e8-8834e7bbcb36\n",
            "Started parsing the file under job_id d489e182-3f8b-4c1a-8fb9-1c1eaebe3445\n",
            "Started parsing the file under job_id 3270f4de-368a-4244-9659-a632e4a329d2\n",
            "Started parsing the file under job_id e3cb6c6e-635e-4911-9f2a-7fd480c277b5\n",
            "Started parsing the file under job_id 0ad16a3b-511b-4155-a3c1-9213bba046e7\n",
            "Started parsing the file under job_id 98345289-5042-4d87-8949-0e296b459b35\n",
            "Started parsing the file under job_id 88c74ae0-3583-4932-acba-9a03c75d8f37\n",
            "Started parsing the file under job_id 11ba80ae-5489-4c0b-a292-5bbf1a63b2b1\n",
            "Started parsing the file under job_id 8cfa538a-c1de-4f3b-bfa2-dbaf4065337a\n",
            "Started parsing the file under job_id 30173982-c706-470b-9d2a-34fce61393df\n",
            "Started parsing the file under job_id 19cc70af-b4cb-46d7-aed5-a47dd052feee\n",
            "Started parsing the file under job_id 19a1bf24-a413-48cf-9058-4ffd019a9180\n",
            "Started parsing the file under job_id b5963e7a-0d41-4603-96cd-95738f4de912\n",
            "Started parsing the file under job_id 9eade2c8-7bd7-4e2f-850a-a63e1302f840\n",
            "Started parsing the file under job_id 940bc43f-16a2-436a-adfa-7ced10fe2e67\n",
            "Started parsing the file under job_id 182e7faa-a3f0-41b1-8e1e-8e5bbdcb9262\n",
            "Started parsing the file under job_id 8cd0a315-4d80-4c3d-a10a-5264fd29027d\n",
            "Started parsing the file under job_id 0668de80-3a3f-4f98-a8ba-5ff45c1a4987\n",
            "Started parsing the file under job_id c110083f-3690-4a11-89ed-00aa434cd2db\n",
            "Started parsing the file under job_id 81cd122c-9b4a-41e7-973b-3d9bb5353f01\n",
            "Started parsing the file under job_id 54358bfe-89fb-41ab-beab-6411d62020a1\n",
            "Started parsing the file under job_id bca5bdaa-ca19-4611-8247-da195b5399eb\n",
            "Started parsing the file under job_id ea609679-2e74-41ff-9376-e9013f8af368\n",
            "Started parsing the file under job_id aa6f99da-b7b0-4ac3-90a6-c650b1e922c3\n",
            "Started parsing the file under job_id fcc71f58-f856-4f72-b32d-a29b460697bb\n",
            "Started parsing the file under job_id d8b00e98-0061-4516-9b52-581e9b9a1fc0\n",
            "Started parsing the file under job_id f4c8b20e-524c-41f4-9f57-4696bf884c81\n",
            "Started parsing the file under job_id 51f3445a-122f-4df6-b873-2bd097749f8d\n",
            "Started parsing the file under job_id abd83a8b-0b45-471c-8408-f9e46a18dbf1\n",
            "Started parsing the file under job_id 0b29a93f-0ccb-4ed1-910b-d47467101349\n",
            "Started parsing the file under job_id 18c66754-c6db-4c46-a32b-741df53c930d\n",
            "Started parsing the file under job_id 525a7dc4-52d4-4161-8cc0-209a2e64d6de\n",
            "Started parsing the file under job_id 9843ef2b-edca-4fd6-8a18-bd66f0fdfa9c\n",
            "Started parsing the file under job_id de2e3cf6-2459-4fcb-a5dd-9a90d8f7e995\n",
            "Started parsing the file under job_id b893632a-57b0-47bf-bd2b-6be5875ff004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSkZuR4VTucN",
        "outputId": "39548b0b-a6bc-4d11-d56d-5f37ba70e954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='a1060dd6-2f0b-4dc7-96c7-667f8130d706', embedding=None, metadata={'file_path': '/content/drive/MyDrive/Omdena_Challenge/new_LK_tea_dataset/Circulers/07_General-Instruction-for-the-Submission-of-Tea-Samples-for-Testing_compressed.pdf', 'file_name': '07_General-Instruction-for-the-Submission-of-Tea-Samples-for-Testing_compressed.pdf', 'file_type': 'application/pdf', 'file_size': 14710, 'creation_date': '2024-11-18', 'last_modified_date': '2024-11-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# ANALYTICAL LABORATORY OF SRILANKA TEA BOARD\\n\\n# GENERAL INSTRUCTION FOR THE SUBMISSION OF TEA SAMPLES FOR TESTING\\n\\n# 1. Submission of tea sample\\n\\n- Sample of tea should be submitted to the Analytical Laboratory on any working day (Monday-Friday) between 8.30 a. m. to 4.15 p.m. (For internal clients) /3.30p.m. (For external clients).\\n- Sample of tea should be submitted to the Analytical Laboratory along with a written request (preferably on an official memo / Company letterhead) addressed to the Director (Analytical Services).\\n- “Levy of Fees & Chargers for Board’s Service” (Ref: OR/1/87) dated 01st September 2011 together with the duly filled AL/T Form at the time submitting the test sample.\\n\\n# 2. Sample Container and size of the tea sample\\n\\n- The sample shall be accepted in a clean, heat sealed air tight aluminium laminated craft paper bag/envelope /Sealed PET bottles / or finally packed product for shipment/dispatch.\\n- Quantity of the tea sample should not be less than 100g (Net Weight) for microbiology /Pesticide residue testing and quantity of the tea sample should not be less than 250g (Net Weight) for Chemical testing.\\n- Laboratory will accept either solid / liquid sample of tea for analysis. If the sample is in the form of liquid, it is Important to contact the Analytical Laboratory official prior to submit the sample.\\n- For specialized testing (i.e. microbiology testing) a sterilized sample cover/s will be issued at the written request of the client/ for external clients after making the payment.\\n\\n# 3. Labeling of tea sample\\n\\n- An identification number (preferably with blend no, STD and grade) should be appeared on each sample packet and such information should comply with the details of the covering letter.\\n\\n# 4. Documents to be submitted\\n\\nThe following information/ documents should provide with the tea sample.\\n\\n# For Internal clients;\\n\\n- a) Official memo with detailed requirement for testing\\n\\n# For external clients;\\n\\n- a) Certified copy of the blend sheet for the respective tea sample\\n- b) Certified copy of the Performa invoice for the tea sample of respective consignment.\\n- c) Certified copy of the CUSDEC sheet for the respective consignment.\\n- d) Certified copies of other relevant documents.\\n\\n# 5. Tracking of sample testing status:\\n\\n- It’s required to provide contact details clearly in the AL/T form to inform the dispatch status of the certificate.\\n- As a guide, approximate time taken for analysis of Chemical Standard -10 working days & for Microbiology Standard – 14 working days.\\n\\nIf you require further information please do not hesitate to contact;\\n\\nDirector (Analytical Services)\\n\\nAnalytical Laboratory,\\n\\nSri Lanka Tea Board,\\n\\nTel/Fax: 011 2581576\\n\\nE-mail: lab@pureceylontea.com\\n\\nIssue No : 1 &nbsp;&nbsp;&nbsp; Issue Date : 24.06.2013\\n\\nISSUED BY : Quality Management Officer\\n\\nRevision No : 1 &nbsp;&nbsp;&nbsp; Revision Date : 01.08.2016 &nbsp;&nbsp;&nbsp; Reference: AL/GI\\n\\nAUTHORIZED BY : Director (Analytical Services)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes to Extract:\n",
        "\n",
        "id_: Unique identifier for the document.\n",
        "embedding: An optional field, which may be None if not set.\n",
        "metadata: A dictionary containing any associated metadata (in this case, it's empty).\n",
        "relationships: A dictionary containing any relationships (empty in this case).\n",
        "text: The content of the document itself.\n",
        "mimetype: The MIME type of the document (in this case, it's 'text/plain').\n",
        "start_char_idx, end_char_idx: These are None in your example, but may hold character index information if applicable.\n",
        "\n",
        "Optional:\n",
        "\n",
        "metadata_template, text_template, and other fields can be ignored if not needed."
      ],
      "metadata": {
        "id": "j1Df3-7nTz_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31mPQqrQLKJu",
        "outputId": "a771ff1d-92e6-4244-b3e0-ee84850e268d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(processed_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUp0TNlBTO70",
        "outputId": "8132a0a2-2d9d-4903-bfd0-471463b446fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(processed_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "IGWBzIqITTfH",
        "outputId": "e585a219-acfb-47c0-8451-51244c8f6981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_index.core.schema.Document"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.schema.Document</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/llama_index/core/schema.py</a>Generic interface for a data document.\n",
              "\n",
              "This document connects to data sources.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 680);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Sample function to convert a single Document to a dictionary\n",
        "def document_to_dict(document: Document):\n",
        "    doc_dict = {\n",
        "        \"id\": document.id_,\n",
        "        \"embedding\": document.embedding,  # May be None\n",
        "        \"metadata\": document.metadata,  # Empty in your case\n",
        "        \"excluded_embed_metadata_keys\": document.excluded_embed_metadata_keys,\n",
        "        \"excluded_llm_metadata_keys\": document.excluded_llm_metadata_keys,\n",
        "        \"relationships\": document.relationships,  # Empty in your case\n",
        "        \"text\": document.text,\n",
        "        \"mimetype\": document.mimetype,\n",
        "        \"start_char_idx\": document.start_char_idx,\n",
        "        \"end_char_idx\": document.end_char_idx,\n",
        "        \"text_template\": document.text_template,\n",
        "        \"metadata_template\": document.metadata_template,\n",
        "        \"metadata_seperator\": document.metadata_seperator\n",
        "    }\n",
        "    return doc_dict\n",
        "\n",
        "# Convert the list of Document objects to a list of dictionaries\n",
        "def documents_to_dicts(documents: list):\n",
        "    return [document_to_dict(doc) for doc in documents]\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "def documents_to_dataframe(documents: list):\n",
        "    # Convert documents list to dicts\n",
        "    doc_dicts = documents_to_dicts(documents)\n",
        "\n",
        "    # Convert list of dictionaries to a DataFrame\n",
        "    df = pd.DataFrame(doc_dicts)\n",
        "    return df\n",
        "\n",
        "\n",
        "# # Sample Document objects (this would normally come from your document processing)\n",
        "# doc1 = Document(\n",
        "#     id_='6af566d2-75f8-4bf1-b97e-a89c9e90066a',\n",
        "#     embedding=None,\n",
        "#     metadata={},\n",
        "#     excluded_embed_metadata_keys=[],\n",
        "#     excluded_llm_metadata_keys=[],\n",
        "#     relationships={},\n",
        "#     text=\"Sample text for document 1\",\n",
        "#     mimetype='text/plain',\n",
        "#     start_char_idx=None,\n",
        "#     end_char_idx=None,\n",
        "#     text_template='{metadata_str}\\n\\n{content}',\n",
        "#     metadata_template='{key}: {value}',\n",
        "#     metadata_seperator='\\n'\n",
        "# )\n",
        "\n",
        "# doc2 = Document(\n",
        "#     id_='8bc31cd1-4c3e-44c9-b6b7-0fa56e987ac0',\n",
        "#     embedding=None,\n",
        "#     metadata={},\n",
        "#     excluded_embed_metadata_keys=[],\n",
        "#     excluded_llm_metadata_keys=[],\n",
        "#     relationships={},\n",
        "#     text=\"Sample text for document 2\",\n",
        "#     mimetype='text/plain',\n",
        "#     start_char_idx=None,\n",
        "#     end_char_idx=None,\n",
        "#     text_template='{metadata_str}\\n\\n{content}',\n",
        "#     metadata_template='{key}: {value}',\n",
        "#     metadata_seperator='\\n'\n",
        "# )\n",
        "\n",
        "# # Assuming we have a list of Document objects\n",
        "# documents = [doc1, doc2]\n",
        "\n",
        "# Convert to DataFrame\n",
        "processed_df = documents_to_dataframe(processed_documents)\n",
        "\n",
        "# # Show the resulting DataFrame\n",
        "processed_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "9CCnZTroUQ_O",
        "outputId": "43fae5ab-6e6e-4ff2-f90d-caf8f22073dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       id embedding  \\\n",
              "0    a1060dd6-2f0b-4dc7-96c7-667f8130d706      None   \n",
              "1    3ab1ff71-ad31-4f45-a673-d8272f3876c4      None   \n",
              "2    f34dac8e-599d-4ef5-befb-c0c9bc51f0dc      None   \n",
              "3    d4492d11-4e1d-4e54-970a-a05d46a7feb2      None   \n",
              "4    830e51b3-e6bb-4239-b478-3fda0ff27adb      None   \n",
              "..                                    ...       ...   \n",
              "620  7fa6f91c-772a-4594-aa17-6f92b8023027      None   \n",
              "621  4e4ea0a1-e4c1-4ecd-acf8-b7933bf324eb      None   \n",
              "622  3dc6d2af-0509-4c7b-83da-36d862a2e037      None   \n",
              "623  7017bcbd-a2f3-4eab-bd30-e4521a5875fb      None   \n",
              "624  8b763449-fd6f-4ef1-86f2-13712d2b355e      None   \n",
              "\n",
              "                                              metadata  \\\n",
              "0    {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "1    {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "2    {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "3    {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "4    {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "..                                                 ...   \n",
              "620  {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "621  {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "622  {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "623  {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "624  {'file_path': '/content/drive/MyDrive/Omdena_C...   \n",
              "\n",
              "                          excluded_embed_metadata_keys  \\\n",
              "0    [file_name, file_type, file_size, creation_dat...   \n",
              "1    [file_name, file_type, file_size, creation_dat...   \n",
              "2    [file_name, file_type, file_size, creation_dat...   \n",
              "3    [file_name, file_type, file_size, creation_dat...   \n",
              "4    [file_name, file_type, file_size, creation_dat...   \n",
              "..                                                 ...   \n",
              "620  [file_name, file_type, file_size, creation_dat...   \n",
              "621  [file_name, file_type, file_size, creation_dat...   \n",
              "622  [file_name, file_type, file_size, creation_dat...   \n",
              "623  [file_name, file_type, file_size, creation_dat...   \n",
              "624  [file_name, file_type, file_size, creation_dat...   \n",
              "\n",
              "                            excluded_llm_metadata_keys relationships  \\\n",
              "0    [file_name, file_type, file_size, creation_dat...            {}   \n",
              "1    [file_name, file_type, file_size, creation_dat...            {}   \n",
              "2    [file_name, file_type, file_size, creation_dat...            {}   \n",
              "3    [file_name, file_type, file_size, creation_dat...            {}   \n",
              "4    [file_name, file_type, file_size, creation_dat...            {}   \n",
              "..                                                 ...           ...   \n",
              "620  [file_name, file_type, file_size, creation_dat...            {}   \n",
              "621  [file_name, file_type, file_size, creation_dat...            {}   \n",
              "622  [file_name, file_type, file_size, creation_dat...            {}   \n",
              "623  [file_name, file_type, file_size, creation_dat...            {}   \n",
              "624  [file_name, file_type, file_size, creation_dat...            {}   \n",
              "\n",
              "                                                  text    mimetype  \\\n",
              "0    # ANALYTICAL LABORATORY OF SRILANKA TEA BOARD\\...  text/plain   \n",
              "1    # MINISTRY OF PLANTATION\\n\\n# SRI LANKA TEA BO...  text/plain   \n",
              "2                                      NO_CONTENT_HERE  text/plain   \n",
              "3    # Sri Lanka Tea Board Minimum Quality Standard...  text/plain   \n",
              "4    # Sri Lanka Tea Board Minimum Quality Standard...  text/plain   \n",
              "..                                                 ...         ...   \n",
              "620  # Definitions\\n\\n” Minister ” means the Minist...  text/plain   \n",
              "621  # PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...  text/plain   \n",
              "622  remuneration as may be specified in the releva...  text/plain   \n",
              "623  # PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...  text/plain   \n",
              "624  # Tea Research Board (Amendment) Act, No. 8 of...  text/plain   \n",
              "\n",
              "    start_char_idx end_char_idx                text_template  \\\n",
              "0             None         None  {metadata_str}\\n\\n{content}   \n",
              "1             None         None  {metadata_str}\\n\\n{content}   \n",
              "2             None         None  {metadata_str}\\n\\n{content}   \n",
              "3             None         None  {metadata_str}\\n\\n{content}   \n",
              "4             None         None  {metadata_str}\\n\\n{content}   \n",
              "..             ...          ...                          ...   \n",
              "620           None         None  {metadata_str}\\n\\n{content}   \n",
              "621           None         None  {metadata_str}\\n\\n{content}   \n",
              "622           None         None  {metadata_str}\\n\\n{content}   \n",
              "623           None         None  {metadata_str}\\n\\n{content}   \n",
              "624           None         None  {metadata_str}\\n\\n{content}   \n",
              "\n",
              "    metadata_template metadata_seperator  \n",
              "0      {key}: {value}                 \\n  \n",
              "1      {key}: {value}                 \\n  \n",
              "2      {key}: {value}                 \\n  \n",
              "3      {key}: {value}                 \\n  \n",
              "4      {key}: {value}                 \\n  \n",
              "..                ...                ...  \n",
              "620    {key}: {value}                 \\n  \n",
              "621    {key}: {value}                 \\n  \n",
              "622    {key}: {value}                 \\n  \n",
              "623    {key}: {value}                 \\n  \n",
              "624    {key}: {value}                 \\n  \n",
              "\n",
              "[625 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1454994b-2584-4c2e-bc47-7fd7c76bd402\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>embedding</th>\n",
              "      <th>metadata</th>\n",
              "      <th>excluded_embed_metadata_keys</th>\n",
              "      <th>excluded_llm_metadata_keys</th>\n",
              "      <th>relationships</th>\n",
              "      <th>text</th>\n",
              "      <th>mimetype</th>\n",
              "      <th>start_char_idx</th>\n",
              "      <th>end_char_idx</th>\n",
              "      <th>text_template</th>\n",
              "      <th>metadata_template</th>\n",
              "      <th>metadata_seperator</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a1060dd6-2f0b-4dc7-96c7-667f8130d706</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># ANALYTICAL LABORATORY OF SRILANKA TEA BOARD\\...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3ab1ff71-ad31-4f45-a673-d8272f3876c4</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># MINISTRY OF PLANTATION\\n\\n# SRI LANKA TEA BO...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f34dac8e-599d-4ef5-befb-c0c9bc51f0dc</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td>NO_CONTENT_HERE</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d4492d11-4e1d-4e54-970a-a05d46a7feb2</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># Sri Lanka Tea Board Minimum Quality Standard...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>830e51b3-e6bb-4239-b478-3fda0ff27adb</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># Sri Lanka Tea Board Minimum Quality Standard...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>7fa6f91c-772a-4594-aa17-6f92b8023027</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># Definitions\\n\\n” Minister ” means the Minist...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>4e4ea0a1-e4c1-4ecd-acf8-b7933bf324eb</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>3dc6d2af-0509-4c7b-83da-36d862a2e037</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td>remuneration as may be specified in the releva...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>7017bcbd-a2f3-4eab-bd30-e4521a5875fb</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>8b763449-fd6f-4ef1-86f2-13712d2b355e</td>\n",
              "      <td>None</td>\n",
              "      <td>{'file_path': '/content/drive/MyDrive/Omdena_C...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>[file_name, file_type, file_size, creation_dat...</td>\n",
              "      <td>{}</td>\n",
              "      <td># Tea Research Board (Amendment) Act, No. 8 of...</td>\n",
              "      <td>text/plain</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{metadata_str}\\n\\n{content}</td>\n",
              "      <td>{key}: {value}</td>\n",
              "      <td>\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>625 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1454994b-2584-4c2e-bc47-7fd7c76bd402')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1454994b-2584-4c2e-bc47-7fd7c76bd402 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1454994b-2584-4c2e-bc47-7fd7c76bd402');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-10b1d532-18ac-4b8b-8b1f-2af48824a29c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10b1d532-18ac-4b8b-8b1f-2af48824a29c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-10b1d532-18ac-4b8b-8b1f-2af48824a29c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3868fa29-198e-416c-80d0-2e1cdc164678\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('processed_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3868fa29-198e-416c-80d0-2e1cdc164678 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('processed_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_df.to_csv('/content/drive/MyDrive/Omdena_Challenge/PREPROCESSING/llama_processed_documents_markdown.csv',index=False)"
      ],
      "metadata": {
        "id": "aCkAGZfFUozR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract metadata\n",
        "this is done from the saved csv, and not all the chunked content we got from llamaparse via a vector database\n",
        "\n",
        "This is discontinuation from the above code, it uses groq directly\n",
        "\n",
        "models do have limits, so cannot feed whole parsed document e.g. markdown file. Will still need chunks, so first and last two (since the very last could be very small in size)"
      ],
      "metadata": {
        "id": "PLjfcoXKRr-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxfiByRzhHOf",
        "outputId": "67d4071a-0641-4044-fcb1-16f6dcaa6aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the DataFrame (assuming the 6th column contains dictionaries with the 'text' key)\n",
        "processed_df = pd.read_csv('/content/drive/MyDrive/Omdena_Challenge/PREPROCESSING/llama_processed_documents_markdown.csv')\n"
      ],
      "metadata": {
        "id": "mPobGkq7jdl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "# Convert the 'metadata' column from string representation to dictionary\n",
        "processed_df['metadata'] = processed_df['metadata'].apply(ast.literal_eval)\n",
        "\n",
        "# Now you can access dictionary keys\n",
        "first_file_name = processed_df['metadata'].iloc[0]['file_name']\n",
        "print(first_file_name)  # Output: file1.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_ww5yo3kLnv",
        "outputId": "de0c7a8d-1c9d-4f9f-f484-872752978f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07_General-Instruction-for-the-Submission-of-Tea-Samples-for-Testing_compressed.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the DataFrame with 'metadata' column containing dictionaries\n",
        "df_metadata = pd.DataFrame()\n",
        "# Extracting the 'filename' from the 'metadata' column\n",
        "df_metadata['file_name'] = processed_df['metadata'].apply(lambda x: x.get('file_name', None))\n",
        "df_metadata['markdown_content'] = processed_df['text']\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUzsBlK9faQY",
        "outputId": "2f8f9178-ecf8-4d57-99a8-e3eeef08d037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             file_name  \\\n",
            "0    07_General-Instruction-for-the-Submission-of-T...   \n",
            "1    01_Sri-Lanka-Tea-Board-Standards-Directives-fo...   \n",
            "2    01_Sri-Lanka-Tea-Board-Standards-Directives-fo...   \n",
            "3    01_Sri-Lanka-Tea-Board-Standards-Directives-fo...   \n",
            "4    01_Sri-Lanka-Tea-Board-Standards-Directives-fo...   \n",
            "..                                                 ...   \n",
            "620                                         ACT_33.txt   \n",
            "621                                         ACT_34.txt   \n",
            "622                                         ACT_34.txt   \n",
            "623                                         ACT_35.txt   \n",
            "624                                         ACT_35.txt   \n",
            "\n",
            "                                      markdown_content  \n",
            "0    # ANALYTICAL LABORATORY OF SRILANKA TEA BOARD\\...  \n",
            "1    # MINISTRY OF PLANTATION\\n\\n# SRI LANKA TEA BO...  \n",
            "2                                      NO_CONTENT_HERE  \n",
            "3    # Sri Lanka Tea Board Minimum Quality Standard...  \n",
            "4    # Sri Lanka Tea Board Minimum Quality Standard...  \n",
            "..                                                 ...  \n",
            "620  # Definitions\\n\\n” Minister ” means the Minist...  \n",
            "621  # PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...  \n",
            "622  remuneration as may be specified in the releva...  \n",
            "623  # PARLIAMENT OF THE DEMOCRATIC SOCIALIST REPUB...  \n",
            "624  # Tea Research Board (Amendment) Act, No. 8 of...  \n",
            "\n",
            "[625 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save parsed text to markdown files in main csv and also in folder because the parsed results have many other things\n",
        "\n"
      ],
      "metadata": {
        "id": "GcV4C-MRSDC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# only pick smaller mardown techunks, first and last two in a separate dataframe"
      ],
      "metadata": {
        "id": "2lFPT2dx6Pnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Group by 'file_name' and apply a custom function to each group\n",
        "def merge_markdown_content(group):\n",
        "\n",
        "    # combined_text_all = first_row['markdown_content'] + \" \" + \" \".join(last_two_rows['markdown_content'])\n",
        "    # df.groupby(['file_name', 'file_path'], as_index=False)['markdown_content'].agg(lambda x: ' '.join(x))\n",
        "    combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n",
        "\n",
        "\n",
        "    # Take the first row\n",
        "    first_row = group.iloc[0]\n",
        "    # Take the last two rows (if there are at least two)\n",
        "    last_two_rows = group.tail(2)\n",
        "\n",
        "    # Combine the first row text_content with the last two rows text_content\n",
        "    combined_markdown_first_last = first_row['markdown_content'] + \" \" + \" \".join(last_two_rows['markdown_content'])\n",
        "\n",
        "\n",
        "    # Return a new series with file_name, file_path and the combined text_content\n",
        "    return pd.Series({\n",
        "        'file_name': first_row['file_name'],\n",
        "        # 'file_path': first_row['file_path'],\n",
        "        'markdown_content_long': combined_markdown_all,\n",
        "        'markdown_content_short': combined_markdown_first_last\n",
        "\n",
        "    })\n",
        "\n",
        "# Apply the function to each group\n",
        "merged_df = df_metadata.groupby('file_name', as_index=False).apply(merge_markdown_content)\n",
        "\n",
        "# Drop the multi-index that `apply` creates\n",
        "merged_df = merged_df.reset_index(drop=True)\n",
        "\n",
        "# View the merged DataFrame\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdmvYjWe_Kj8",
        "outputId": "deaba239-1951-49dd-b154-cea10b241e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2697a585077a>:9: FutureWarning: using <function merge_markdown_content.<locals>.<lambda> at 0x7a0175cf6710> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
            "  combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n",
            "<ipython-input-8-2697a585077a>:9: FutureWarning: using <function merge_markdown_content.<locals>.<lambda> at 0x7a0175cf5c60> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
            "  combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n",
            "<ipython-input-8-2697a585077a>:9: FutureWarning: using <function merge_markdown_content.<locals>.<lambda> at 0x7a0175cf5d80> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
            "  combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n",
            "<ipython-input-8-2697a585077a>:9: FutureWarning: using <function merge_markdown_content.<locals>.<lambda> at 0x7a0175cf4b80> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
            "  combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             file_name  \\\n",
            "0    01_Sri-Lanka-Tea-Board-Standards-Directives-fo...   \n",
            "1    07_General-Instruction-for-the-Submission-of-T...   \n",
            "2                                       1998 10 30.pdf   \n",
            "3                                        2158-03_E.pdf   \n",
            "4                                        2245-29_E.pdf   \n",
            "..                                                 ...   \n",
            "163                           al-mqs-08 2008 04 24.pdf   \n",
            "164  guidelines_on_covid-19_in_tea_plantations_eng_...   \n",
            "165                                      mf bl 119.pdf   \n",
            "166                                       scan0002.pdf   \n",
            "167             tc - e - vi - 68 -vol 2 2003 03 07.pdf   \n",
            "\n",
            "                                 markdown_content_long  \\\n",
            "0    1    # MINISTRY OF PLANTATION\\n\\n# SRI LANKA T...   \n",
            "1    0    # ANALYTICAL LABORATORY OF SRILANKA TEA B...   \n",
            "2    331    # Sri Lanka Tea Board\\n\\n574, Galle Roa...   \n",
            "3    309    # PART I : SEC. (I) - GAZETTE EXTRAORDI...   \n",
            "4    308    # PART I : SEC. (I) - GAZETTE EXTRAORDI...   \n",
            "..                                                 ...   \n",
            "163  332    # SRI LANKA TEA BOARD\\n\\n# Circular No....   \n",
            "164  201    # TEA RESEARCH INSTITUTE OF SRI LANKA\\n...   \n",
            "165  352    # Sri Lanka Tea Board\\n\\n# Tea Commissi...   \n",
            "166  391    # TKANSLATION\\n\\n# REGISTERED POST\\n\\n#...   \n",
            "167  395    # SRI LANKA TEA BOARD\\n\\nRef: TCIENV68-...   \n",
            "\n",
            "                                markdown_content_short  \n",
            "0    # MINISTRY OF PLANTATION\\n\\n# SRI LANKA TEA BO...  \n",
            "1    # ANALYTICAL LABORATORY OF SRILANKA TEA BOARD\\...  \n",
            "2    # Sri Lanka Tea Board\\n\\n574, Galle Road, Colo...  \n",
            "3    # PART I : SEC. (I) - GAZETTE EXTRAORDINARY OF...  \n",
            "4    # PART I : SEC. (I) - GAZETTE EXTRAORDINARY OF...  \n",
            "..                                                 ...  \n",
            "163  # SRI LANKA TEA BOARD\\n\\n# Circular No.: AL/MQ...  \n",
            "164  # TEA RESEARCH INSTITUTE OF SRI LANKA\\n\\nIssue...  \n",
            "165  # Sri Lanka Tea Board\\n\\n# Tea Commissioner Di...  \n",
            "166  # TKANSLATION\\n\\n# REGISTERED POST\\n\\n# SRI LA...  \n",
            "167  # SRI LANKA TEA BOARD\\n\\nRef: TCIENV68-Vol m\\n...  \n",
            "\n",
            "[168 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2697a585077a>:9: FutureWarning: using <function merge_markdown_content.<locals>.<lambda> at 0x7a0175cf4dc0> in Series.agg cannot aggregate and has been deprecated. Use Series.transform to keep behavior unchanged.\n",
            "  combined_markdown_all = group['markdown_content'].agg(lambda x:''.join(x))\n",
            "<ipython-input-8-2697a585077a>:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  merged_df = df_metadata.groupby('file_name', as_index=False).apply(merge_markdown_content)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "# os.environ['LLAMA_CLOUD_API_KEY'] = getpass.getpass('Enter your LLamacloud API Key: ')\n",
        "\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = getpass.getpass('Enter your GROQ API Key: ')\n",
        "groq_api=os.environ['GROQ_API_KEY']"
      ],
      "metadata": {
        "id": "zhaOZUhArpbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125c6b6b-ecf9-431d-ad82-b2b3210efec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Assuming Groq API key is set up properly\n",
        "\n",
        "\n",
        "client = Groq(api_key=groq_api)\n",
        "\n",
        "def clean_text_for_metadata_extraction(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess the extracted text content to make it suitable for metadata extraction.\n",
        "    This will remove unwanted text such as 'Copyright' and any excessive spaces or line breaks.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "\n",
        "\n",
        "    # Remove specific unwanted text like 'Copyright'\n",
        "    # Remove specific unwanted text like 'Copyright' (case-insensitive) and everything after it\n",
        "    text = re.sub(r'copyright[\\s\\S]*?(\\n|\\r|\\Z)', '', text, flags=re.IGNORECASE)  # Remove copyright section and text after it\n",
        "\n",
        "    # # Clean up the text by replacing line breaks and excessive spaces\n",
        "    # text = re.sub(r'\\n+', ' ', text)  # Replace multiple line breaks with a single space\n",
        "    # text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    # text = text.strip()  # Remove leading/trailing spaces\n",
        "\n",
        "    # # Optionally, trim to the first 500 characters for better performance\n",
        "    # return text[:500]\n",
        "    return text\n",
        "\n",
        "def extract_metadata(row):\n",
        "    \"\"\"\n",
        "    Extract title and published date from the document's text using the Groq API.\n",
        "    The prompt is designed to instruct the Groq model to return a JSON object containing these two pieces of information.\n",
        "    \"\"\"\n",
        "    text =  row['markdown_content_short']\n",
        "\n",
        "    # Clean and preprocess the text before passing to the Groq API\n",
        "    cleaned_text = clean_text_for_metadata_extraction(text)\n",
        "    if not cleaned_text:\n",
        "        return None\n",
        "\n",
        "    prompt = (\n",
        "        \"Please process the following document and extract the relevant details in English in JSON format:\"\n",
        "        \"- **parsed_title**: Provide the main document title,; if not, indicate 'None'.\"\n",
        "        # \"- **Laws/Acts/Amendments**: Identify any laws, acts, or amendments mentioned in the document, including their date.\\n\"\n",
        "        # \"- **Topics**: List the key topics discussed in the document.\\n\"\n",
        "        \"- **parsed_issue_date**: Provide the document's issuance date in DD.MM.YYYY format; if not, indicate 'None'.\"\n",
        "        \"- **parsed_reference_number**: If available, provide the reference number; if not, indicate 'None'.\"\n",
        "        \"Document text:\\n\"\n",
        "        f\"{cleaned_text}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Send the prompt to the Groq API\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"llama-3.2-1b-preview\",  # Make sure this model is appropriate for your task\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": \"```json\"}\n",
        "            ],\n",
        "            stop=\"```\",\n",
        "        )\n",
        "\n",
        "        # Try to extract the JSON response\n",
        "        return completion.choices[0].message.content.strip(\"```json\").strip()\n",
        "\n",
        "    except (AttributeError, IndexError) as e:\n",
        "        print(f\"Error processing row: {e}\")\n",
        "        return None\n",
        "\n",
        "# Apply the metadata extraction to the DataFrame\n",
        "merged_df['metadata_returned'] = merged_df.apply(extract_metadata, axis = 1)\n",
        "\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "h844kMuQOLvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bu_merged_df = merged_df.copy()"
      ],
      "metadata": {
        "id": "N9cfd4zdnxaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = bu_merged_df.copy()"
      ],
      "metadata": {
        "id": "q2u-BjLVD_dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "GTMGPTF2GdFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using iloc to get the first row's metadata column value\n",
        "first_metadata_value = merged_df.iloc[0]['metadata_returned']\n",
        "first_metadata_value\n"
      ],
      "metadata": {
        "id": "rVmxDf4SniBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "# Function to clean and parse the metadata string\n",
        "def clean_json_string(json_string):\n",
        "    try:\n",
        "        # Check if the string is empty or contains invalid data\n",
        "        if not json_string or json_string.isspace():\n",
        "            print(\"Empty string or whitespace found\")\n",
        "            return None\n",
        "\n",
        "        # Remove unwanted characters (non-ASCII, newline, etc.)\n",
        "        cleaned_string = re.sub(r'[^\\x00-\\x7F]+', ' ', json_string)  # Remove non-ASCII characters\n",
        "        cleaned_string = re.sub(r'[^a-zA-Z0-9.,;:?!()\"\\'\\s/-]', '', cleaned_string)  # Remove non-English characters\n",
        "        cleaned_string = cleaned_string.replace('<|end_header_id|>', '')  # Remove any specific unwanted tags\n",
        "        cleaned_string = cleaned_string.replace('\\n', ' ').replace('\\r', ' ').strip()  # Remove newlines and extra spaces\n",
        "\n",
        "        # Check for multiple JSON objects or broken JSON (simple check)\n",
        "        if cleaned_string.count('{') > 1:\n",
        "            # If there are multiple objects, extract the first valid one\n",
        "            start = cleaned_string.find('{')\n",
        "            end = cleaned_string.rfind('}') + 1\n",
        "            cleaned_string = cleaned_string[start:end]\n",
        "\n",
        "        # Try to load the cleaned string as JSON\n",
        "        json_object = json.loads(cleaned_string)  # Try parsing the cleaned string as JSON\n",
        "\n",
        "        return json_object\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        return None  # Return None if JSON is malformed\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        return None  # Handle any other unexpected errors\n",
        "\n",
        "# Apply the cleaning function to the 'metadata_returned' column\n",
        "merged_df['metadata_returned'] = merged_df['metadata_returned'].apply(clean_json_string)\n",
        "\n",
        "# Expand the 'metadata_returned' column into individual columns in place\n",
        "merged_df = pd.concat([merged_df, merged_df['metadata_returned'].apply(pd.Series)], axis=1)\n",
        "\n",
        "# Drop the original 'metadata_returned' column to avoid duplication\n",
        "merged_df.drop(columns=['metadata_returned'], inplace=True)\n",
        "\n",
        "# Display the expanded DataFrame\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "id": "-DUSn4tkKvb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# works but misses some rows\n",
        "# import pandas as pd\n",
        "# import json\n",
        "# import re\n",
        "\n",
        "\n",
        "\n",
        "# # Function to clean and parse the metadata string\n",
        "# def clean_json_string(json_string):\n",
        "#     try:\n",
        "#         # Check if the string is empty or contains invalid data\n",
        "#         if not json_string or json_string.isspace():\n",
        "#             print(\"Empty string or whitespace found\")\n",
        "#             return None\n",
        "\n",
        "#         # Remove unwanted characters (non-ASCII, newline, etc.)\n",
        "#         cleaned_string = re.sub(r'[^a-zA-Z0-9.,;:?!()\"\\'\\s/-]', '', json_string)  # Remove non-English characters\n",
        "#         cleaned_string = re.sub(r'[^\\x00-\\x7F]+', ' ', json_string)  # Remove non-ASCII characters\n",
        "#         cleaned_string = cleaned_string.replace('<|end_header_id|>', '')  # Remove any specific unwanted tags\n",
        "#         cleaned_string = cleaned_string.replace('\\n', ' ').replace('\\r', ' ').strip()  # Remove newlines and extra spaces\n",
        "\n",
        "#         # Check for multiple JSON objects or broken JSON (simple check)\n",
        "#         if cleaned_string.count('{') > 1:\n",
        "#             # If there are multiple objects, extract the first valid one\n",
        "#             start = cleaned_string.find('{')\n",
        "#             end = cleaned_string.rfind('}') + 1\n",
        "#             cleaned_string = cleaned_string[start:end]\n",
        "\n",
        "#         # Try to load the cleaned string as JSON\n",
        "#         json_object = json.loads(cleaned_string)  # Try parsing the cleaned string as JSON\n",
        "\n",
        "#         return json_object\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         print(f\"Error decoding JSON: {e}\")\n",
        "#         return None  # Return None if JSON is malformed\n",
        "#     except Exception as e:\n",
        "#         print(f\"Unexpected error: {e}\")\n",
        "#         return None  # Handle any other unexpected errors\n",
        "\n",
        "# # Apply the cleaning function to the 'metadata_returned' column\n",
        "# merged_df['metadata_returned'] = merged_df['metadata_returned'].apply(clean_json_string)\n",
        "\n",
        "# # Remove rows where parsing failed (i.e., cleaned string is None)\n",
        "# merged_df = merged_df.dropna(subset=['metadata_returned'])\n",
        "\n",
        "\n",
        "# # Expanding the 'metadata_returned' column into individual columns in place\n",
        "# merged_df = pd.concat([merged_df, merged_df['metadata_returned'].apply(pd.Series)], axis=1)\n",
        "\n",
        "# # Drop the original 'metadata_returned' column to avoid duplication\n",
        "# merged_df.drop(columns=['metadata_returned', 'title','issue_date','reference_number'], inplace=True)\n",
        "\n",
        "# # Display the expanded DataFrame\n",
        "# merged_df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WtBNVG60CJsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Save the updated DataFrame with the new columns to a new CSV file\n",
        "merged_df.to_csv('/content/drive/MyDrive/Omdena_Challenge/PREPROCESSING/llama_parsed_metadata.csv', index=False)\n"
      ],
      "metadata": {
        "id": "roTm4ImLiYnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By now I have a parsed result json (keep) and a metadata df, no need to save it\n",
        "# append metadata results to main csv"
      ],
      "metadata": {
        "id": "j7-QEJ2l2MLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: append merged_df column based on filename to all_data and add columns metadata in full by extracting key values from dictionary and appending as columns and markdown_content_full\n",
        "\n",
        "# Assuming 'all_data' is your main DataFrame and 'merged_df' is the DataFrame you created with the merged text content.\n",
        "# Also assuming 'merged_df' has columns 'file_name', 'markdown_content_long', 'markdown_content_short'\n",
        "\n",
        "def append_merged_df_to_all_data(all_data, merged_df):\n",
        "  \"\"\"\n",
        "  Appends the merged_df columns based on the 'file_name' to the all_data DataFrame.\n",
        "  \"\"\"\n",
        "  # all_data = all_data.merge(merged_df[['file_name', 'markdown_content_long', 'parsed_title','parsed_issue_date', 'parsed_reference_number']], on='file_name', how='left')\n",
        "  all_data = all_data.merge(merged_df[['file_name', 'markdown_content_long', 'Title','Issue Date', 'Reference Number']], on='file_name', how='left')\n",
        "\n",
        "  return all_data\n",
        "\n",
        "\n",
        "\n",
        "# 1. Append merged_df columns to all_data\n",
        "all_data = append_merged_df_to_all_data(all_data, merged_df)\n",
        "\n",
        "# Print to confirm\n",
        "print(all_data.head())"
      ],
      "metadata": {
        "id": "fuS29lFz4atQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get rows with no content here, and mark as bad quality in original csv, and remove for next step in next code"
      ],
      "metadata": {
        "id": "WmvO2Ix9fWYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get markdown content rows where only NO CONTENT HERE exists and remove the rows\n",
        "\n",
        "def remove_no_content_rows(df):\n",
        "  \"\"\"Removes rows from a DataFrame where 'markdown_content_short' contains only 'NO CONTENT HERE'.\"\"\"\n",
        "  df = df[~df['markdown_content_short'].str.contains(r'^NO_CONTENT_HERE$', na=False)]\n",
        "  return df\n",
        "\n",
        "# Assuming 'merged_df' is the DataFrame with the 'markdown_content_short' column\n",
        "merged_df = remove_no_content_rows(merged_df)\n",
        "\n",
        "# Now 'merged_df' has rows with only 'NO CONTENT HERE' removed."
      ],
      "metadata": {
        "id": "t385K4cPfWBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save main csv"
      ],
      "metadata": {
        "id": "BG86ADEcF3Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Save the updated DataFrame with the new columns to a new CSV file\n",
        "merged_df.to_csv('/content/drive/MyDrive/Omdena_Challenge/PREPROCESSING/v2_new_LK_tea_dataset_updated.csv', index=False)"
      ],
      "metadata": {
        "id": "8pYEWeV_F2rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# estimating number of token from markdown file"
      ],
      "metadata": {
        "id": "XVpMmivY4muQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# each chunk will be below 3,000? 30,000? via llamaparse i guess\n",
        "import pandas as pd\n",
        "\n",
        "# Function to count words and characters in text\n",
        "def count_words_and_characters(text):\n",
        "    words = text.split()  # Split by whitespace to get words\n",
        "    num_words = len(words)\n",
        "    num_characters = len(text)\n",
        "    return num_words, num_characters\n",
        "\n",
        "# Function to count words and characters for the 'text_content' column in the DataFrame\n",
        "def count_stats_in_dataframe(df):\n",
        "    total_words = 0\n",
        "    total_characters = 0\n",
        "\n",
        "    # Iterate over the 'text_content' column in the DataFrame\n",
        "    for text in df['markdown_content_long'].dropna():  # Drop any NaN values in text_content\n",
        "        num_words, num_characters = count_words_and_characters(text)\n",
        "        total_words += num_words\n",
        "        total_characters += num_characters\n",
        "\n",
        "    return total_words, total_characters\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Count words and characters\n",
        "    total_words, total_characters = count_stats_in_dataframe(all_data)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"\\nTotal Stats for 'text_content' column in DataFrame:\")\n",
        "    print(f\"Total Words: {total_words}\")\n",
        "    print(f\"Total Characters: {total_characters}\")\n"
      ],
      "metadata": {
        "id": "Z04_IX0QAoQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}